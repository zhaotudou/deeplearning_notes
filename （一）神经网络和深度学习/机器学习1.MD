机器学习：
机器学习研究的是计算机怎样模拟人类的学习行为，以获取新的知识或技能，并重新组织已有的知识结构使之不断改善自身。简单的说，就是计算机从数据中学习规律和模式，以应用在新数据上做预测的任务。


### （一）深度学习概念
深度学习指的是训练神经网络，有时候规模很大。
#### linear regression 线性回归

回归函数，例如在最简单的房价预测中，我们有几套房屋的面积以及最后的价格，根据这些数据来预测另外的面积的房屋的价格，根据回归预测，在以房屋面积为输入x，输出为价格的坐标轴上，做一条直线最符合这几个点的函数，将它作为根据面积预测价格的根据，这条线就是回归线，对应回归函数。当然也应该有限制条件，价格总不能为0，所以直线不合适，在最初的阶段，应该是为0的横线，这样的经过修改的更符合实际的就可以作为判断依据了。这是一个最简单的神经网络了，只经过了一层也就是一个神经元，从输入面积，经过神经元到输出价格。

在很多神经网络的文献中，都会看到这样的函数，函数一开始是0，然后就是一条直线，这个函数就被称为ReLU函数：修正线性单元函数。rectified linear unit。“修正”指的是取不小于0的值，这是一个单神经元网络，规模很小的神经网络，大一点的神经网络是把这些单个神经元堆叠起来形成的，形成一个更大的神经网络。

假设现在预测房价不只是考虑大小，还有卧室数量，邮编，以及富裕程度，根据这是个条件来预测价格呢，那么就会形成一个更大的神经网络，如图。
![房价预测神经网络](http://upload-images.jianshu.io/upload_images/1779926-81f027027160fbca.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
在图中，x为4个特征输入，y为输出结果房价，中间的为隐藏层，其中第一层挨着x输入的为输入层，每个神经元都与4个输入特征有联系，把这些独立的神经单元堆叠起来，简单的预测期（神经元）形成一个更大的。神经网络的一部分神奇之处在于，当你实现了它之后，你要做的只是输入x，就能得到输出，不管训练集有多大，所有的中间过程，都会自己完成。要做的就是，这有四个输入的神经网络，输入的特征可能是面积，大小等等，已知这些输入的特征，神经网络的工作就是预测对应的价格。输入层的连接数最高，因为每个输入都连接到了中间的每个圆圈。神经网络只要你给足够多的数据关于x和y的数据，给到足够多的训练数据，神经网络非常擅长于计算从x到y的精准映射函数。
神经网络给了输入以及输出的训练数据，是一种监督学习。

几乎所有由神经网络创造的经济价值，都基于其中一种机器学习，我们称之为监督学习，supervisor learning。在监督学习中，输入x，习德一个函数，映射到输出y。这里有一些其他例子，这些例子中神经网络效果超群。也许通过深度学习获利最大的就是在线广告，给网站输入广告信息，网站会考虑是否给你看这个广告，有时还需要输入一些用户信息。神经网络在预测你是否会点击这个广告方面，已经表现的很好，通过向你展示，向用户展示最有可能点开的广告。以及计算机视觉，你输入一个图像，然后想输出一个指数，可以是从1到1000来表明这些照片是1000个不同的图像中的某一个，可以用来给照片打标签。还有语音识别，你可以把一段音频输入神经网络，可以输出文本。机器翻译进步也很大，输入英语句子，直接输出一个中文句子。在无人驾驶技术中，你输入一副图像，汽车前方的一个快照，还有一些雷达信息，基于这个，训练过的神经网络能告诉你路上其他汽车的位置，这是无人驾驶系统的关键组件。要机智的选择x和y，才能解决特定问题，然后把这个监督学习过的组件嵌入到更大型的系统中，比如无人驾驶。可以看出稍微不同的神经网络应用到不同的地方，都行之有效。图像领域里，我们经常用的是卷积神经网络，cnn。对于序列数据，例如音频中含有时间成分，音频是随着时间播放的，所以音频很自然的被表示为以为时间序列。对于序列数据，经常使用RNN，循环神经网络，recurrent neural network。语言最自然的表示方式也是序列数据，语言，汉语，英语，单词和字母都是按序列出现的，更复杂的RNNs经常会用于这些应用。对于更复杂的无人驾驶，输入一张快照，需要使用卷积神经网络架构去处理，雷达信息更不一样，可能需要更复杂的混合的神经网络结构。

#### 结构化数据和非结构化数据
所以，为了更好的说明标准的CNN和RNN结构，所以在文献中，你可以看到标准神经网络和卷积神经网络，卷积网络通常用于图像处理。循环神经网络很好的处理一位时间序列数据，其中包含时间成分。机器学习还可能运用于结构化数据和非结构化数据。
结构化数据：数据的数据库，比如在房价预测中，你可能有一个数据库或者数据列，告诉你房间的大小和卧室的数量，这就是结构化数据，意味着每个特征都有着清晰的定义。
非结构化数据：比如音频，原始音频，图像，你要是被图像或文本中的内容，这里的特征，可能是图像中的像素值，或者文本中的每个单词，非结构化数据相对于结构化数据，计算机理解起来更难。

#### 神经网络应用
神经网络的发展，就是让计算机更好的能理解非结构化数据。语音识别，图像识别，自然语言文本处理。神经网络在非结构化数据上的成功，尤其是媒体。神经网络在很多短期经济价值的创造是基于结构化数据的，比如更好的广告系统，更好的获利建议，有更好的能力去处理很多公司拥有的海量数据库，并且这些数据有准确的预测未来的能力。
![监督学习神经网络应用领域](http://upload-images.jianshu.io/upload_images/1779926-53489903370cce17.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![三种神经网络：标准神经网络   卷积神经网络   循环神经网络](http://upload-images.jianshu.io/upload_images/1779926-fd9b0c248f2f1b90.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

#### 神经网络流行原因
![神经网络发展的原因](http://upload-images.jianshu.io/upload_images/1779926-4fc03112c13b303c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
在前面数据量很小的时候，算法之间性能差异不是很明显。到了大数据段，神经网络的作用就很明显了。训练出一个很好的神经网络，重要的条件是，规模大的数据量m（横轴），以及很好的计算训练能力。

![规模促进了机器学习](http://upload-images.jianshu.io/upload_images/1779926-d648f46edf7a784d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

关于机器学习发招，重要的原因，海量数据，计算速度，算法改进。
关于算法改进，许多算法方面的创新都是为了让神经网络运行的更快，举一个例子，神经网络方面一个巨大的突破是，从sigmoid函数转换到这样的ReLU函数，使用sigmoid函数机器学习的问题是，在这个函数后部分，sigmoid函数的斜率梯度会接近0，所以学习会变得非常缓慢，因为用梯度下降法时，梯度接近0时，参数会变化的很慢，学习也会变的很慢，而通过改变激活函数，神经网络用ReLU函数修正线性单元函数，它的梯度对于所有为正值的输入输出都是1，因此梯度不会逐渐趋近于0。为这里的梯度，这条线的斜率，在这左边是0，我们发现，只需将sigmoid函数转换为ReLU函数便能够使得"梯度下降法"运行的更快，这就是一个例子关于算法创新。其目的就是增加计算速度。

### （二）神经网络基础
#### 2.1 二分分类
当你要构建一个神经网络，有些技巧是相当重要的。例如，m个样本的训练集，你可能会习惯性的去用一个for循环来遍历这m个样本，但事实上，实现一个神经网络，如果你要遍历整个数据集，并不需要直接使用for循环。还有就是，神经网络的计算过程中中，通常有一个正向过程，或者正向传播步骤，接着会有一个反向过程，也叫反向传播步骤。

（1）二分分类案例：
使用logistic回归来阐述，以便能更好的理解。
logistic回归是一个用于二分分类的算法。例如举个例子，输入一张图像，我们要输入y，是猫1或者不会猫0。计算机保存一张图片，要保存三个独立矩阵，分别对应图片中的红绿蓝三个颜色通道，如果输入图片是`64*64`像素的，就有三个`64*64`的矩阵,分别对应图片中的红绿蓝三个像素的亮度，为了方便表示，这里用三个小矩阵`5*4`的来表示，要把这些像素亮度值放进一个特征向量中，就要把这些像素值都提出来，放入一个特征向量x。为了把这些像素值取出来放入特征向量，就要像下面一样定义一个特征向量x以表示这张图片，我们把所有的像素值都取出来，把三层矩阵上的所有数都放入一个向量中，最后得到一个很长的特征向量，把图片中所有的红绿蓝像素强度值都列出来。如果图片是`64*64`的，那么向量x的总维度为`64*64*3`，因为这是三个矩阵的元素数量，乘出来结果为12288,我们用nx=12288来表示输入特征向量x的维度，有时候为了简洁，我们直接用小写的n，来表示特征向量的维度。


![案例，判断图片是否有猫](http://upload-images.jianshu.io/upload_images/1779926-5e221cd4ec5ea523.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![输入特征x的表示](http://upload-images.jianshu.io/upload_images/1779926-17c574efe016e86d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


在二分分类问题中，目标就是训练出一个分类器，它以图片的特征向量x作为输入，预计输出的结果标签y是1还是0，也就是说，预测图片中是否有猫。

（2）相关符号：
后面课程中需要用到的符号。
用一对(x,y)来表示一个独立的样本，x是nx维的特征向量，标签y，值为1或0，训练集由m个训练样本构成，`（x^(1),y^(1)）`表示样本一的输入和输出，`（x^(2),y^(2)）`表示样本二的输入和输出，`（x^(m),y^(m)`）表示最后一个样本m的输入和输出，`{（x^(1),y^(1)）....（x^(m),y^(m)）}`这些一起就表示整个训练集。m表示整个训练集的样本数，有时候为了强调这是训练样本的个数，可以写作`m=m_train`，当说到测试集时，可以用m_test来表示测试集的样本数，最后用更紧凑的符号表示训练集，定义一个矩阵，用大写X来表示，它由训练集中的x1，x2这些组成，像这样写成矩阵的列。每个训练集`x^(1)..x^(m)`则分别为这个矩阵的1到m列，所以这个矩阵有m列，m是训练集的样本数，这个矩阵的高度记为nx。要注意的是，有时候X的定义，训练数据作为行向量堆叠，而不是这样的列向量堆叠。但是构建神经网络时，用列向量堆叠这个约定形式，会让构建过程简单的多。总结一下，X是一个`nx*m`的矩阵，当用python实现时，会看到`X.shape`，这是一条python命令，用来输出矩阵的维度，即`（nx，m）`，表示X是一个`nx*m`的矩阵，这就是如何将训练样本，即输入x用矩阵表示，那输出标签y呢，同样为了简单的构建一个神经网络，将y标签也放入列中，`Y = [y^(1),y^(2),...y^(m)]`,这里的Y是一个`1*m`的矩阵，同样的，在python里面，Y.shape等于（1，m）。后面的课程中你会发现，好的习惯符号能够将不同训练样本的数据联系起来，这里说的数据，不仅有x和y还会有其他的量。将不同的训练样本数据取出来，放到不同的列上，就像刚才处理x和y一样。在logistic回归和神经网络，要用到的符号就是这些了。

![符号表示](http://upload-images.jianshu.io/upload_images/1779926-9157d23a44721b36.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


#### 2.2 logistc回归
这是一个学习算法，用在监督学习中，输出y标签是0或1时，这是一个二分分类的问题。已知输入特征向量是x，可能是一张图，你希望把识别出这是不是猫图，你需要一个算法，可以给出一个预测值，y hat，就是你对y的预测。更正式的说，你希望y hat是一个概率，当输入特征满足条件时，y就是1，所以换句话说，如果x是图片，例如之前判读图片是否有猫的例子，你希望y是一张图片是猫图的概率，x是一个nx维的向量，已知logistic回归的参数是w，也是一个nx维向量，而b就是一个实数，所以已知输入x和参数w和b，如何计算y hat。
也许可以尝试，y = w^T * x + b, 一个输入x的线性函数，事实上，如果你做线性回归，就是这么算的，但这并不是一个非常好的二元分类算法，因为你希望y hat 是y=1的概率，而不是计算y的值，所以y hat应该介于0到1之间，但实际上很难实现，因为`w^T*x+b`可能比1大得多，甚至是负值，这样的概率是没有意义的。所以在logistc回归中，我们把输出变成y hat等于sigmoid函数作用到这个量上，这就是sigmoid函数的图形，横轴是Z，函数是从0到1的光滑函数，与数轴的交点在0.5处，这就是sigmoid(z)的图形，用z来表示w^T * x + b这个量。

![sigmoid函数公式](http://upload-images.jianshu.io/upload_images/1779926-b421211683345106.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


![sigmoid函数图像](http://upload-images.jianshu.io/upload_images/1779926-1063a67429d50d6e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

要注意的是，如果z非常大，那么e^(-z)就很接近0，那么sigmoid(z)就是1/（1+某个接近0的量），所以这就接近1。相反如果z非常小，或者是非常大的负数，那么sigmoid(z)就很接近于0。 

所以当你要实现logistic回归时，你要做的是学习参数w和b，这样y hat 就变成了比较好的估计。对y=1的比较好的估计。当我们对神经网络编程时，我们通常会把w和b分开，这里b对应一个拦截器，在其他课程中，可能看到过不同的表示。在一些符号约定中，定义一个额外的特征向量x0并且等于1，所以出现x就是R^(nx+1)维向量，然后将y hat定义为`σ(θ^T*X)`，在这种符号定义中，出现了一个向量参数θ,这个θ是由向量[θ0，θ1，θ2...θnx]组成的列向量。所以θ0就是扮演的b的角色，这是一个实数，而θ1直到θnx的作用和w一样。事实上，当你实现你的神经网络时，将b和w看成独立的参数可能更好。

![logistic回归模型与参数，以及sigmoid函数](http://upload-images.jianshu.io/upload_images/1779926-16370652616728eb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![参考资料1](http://upload-images.jianshu.io/upload_images/1779926-0f9d2835f960e0d6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


![参考资料2](http://upload-images.jianshu.io/upload_images/1779926-56dad4813fdec20f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

#### 2.3 logistc回归损失函数
为了训练logistic回归模型的参数w和b，需要定义一个成本函数。为了让模型通过学习调整参数要给一个m个样本的训练集，通过训练集，找到参数b和w，来得到你的输出，在训练集上得到预测值，是预测值y hat(i) 更接近于训练集得到的y^(i)。
为了让上面的方程更详细一点，需要说明上面这里定义的y hat（即y头上有个^这个符号），是对一个训练样本x来说的，当然是对每个训练样本，我们使用这些带有圆角的符号方便引用说明，还有区分样本。你的训练样本(i),对应的预测值为通过sigmoid函数获得的y hat^(i),通过函数作用到`W^T*x(i)+b`得到的，所以符号约定就是这个上标(i)来指明数据。

loss function： 损失函数，也叫作误差函数，他们可以用来衡量算法的运行情况，你可以定义损失为y hat和真的类标y的差平方的一半，但通常在logistic回归中，大家都不这么做，因为当你学习这些参数时候，会发现优化问题会变成非凸的，最后会得到很多个局部最优解，所以梯度下降法(gradient descent)可能找不到全局最优值。我们通过定义损失函数L来衡量你的预测输出值y hat和y的实际值有多接近。误差平方看起来似乎是一个合理的选择，但是用这个的话，梯度下降法就不会很有效。在logistic回归中，我们会定义一个不同的损失函数，它有着与误差平方相似的作用，这会给我们一个凸的优化问题，就很容易去做优化。在logistic回归中，我们用到的损失函数如下：
![logstic回归中用到的损失函数L](http://upload-images.jianshu.io/upload_images/1779926-e882c1aec328d248.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
直观地看看为什么这个损失函数能起作用，假设我们使用误差平方越小越好，对于这个logistic回归的损失函数，同样的我们也让它尽可能的小。

![损失函数L](http://upload-images.jianshu.io/upload_images/1779926-2b38fe95c77bb7e6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

1. 当y=1时，我们要使y hat尽可能大，但这是由sigmoid函数估计出来的值，不会超过1，所以要尽可能接近1
2. 当y=0时，要使y hat尽可能小，但是不会小于0，所以尽可能接近0。

损失函数是在单个训练样本中定义的，它衡量了在单个训练样本中的表现。接下来要定义一个成本函数cost function，它衡量的是在全体训练样本上的表现，这个成本函数J根据之前得到的两个参数w和b，J(w,b)
即所有训练样本的损失函数和。而y hat是用一组特定的参数w，b通过logistc回归算法得出的预测输出值。从而成本函数把L带入得到以下形式：
![成本函数J(w,b)最终形式](http://upload-images.jianshu.io/upload_images/1779926-0d188e9eb4014435.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

损失函数只适用于像这样的单个训练样本，而成本函数基于参数的总成本，所以在训练logistic回归模型时，我们要找到合适的参数w和b，让这里的成本函数J尽可能的小。
综上所述，可以得到logistic回归可以被看成一个非常小的神经网络。

#### 2.4 梯度下降法
如何使用梯度下降法来训练或学习训练集上的参数w和b，成本函数只能用来说明你选择的w和b作用效果好不好，最后误差大不大，参数w和b在训练集上的效果，但是我们不知道一个w和b的关系，如何取值，发现一组不好后如何确定下一组取值，这就需要梯度下降法来研究如何训练w和b使得成本函数最小。
![梯度下降法](http://upload-images.jianshu.io/upload_images/1779926-84f59d890c78579a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
这个图中的横轴表示空间参数w和b，在实践中，w可以是更高维的，但为了方便绘图，我们让w是一个实数，b也是一个实数，成本函数J(w,b)是在水平轴上w和b上的曲面，曲面的高度J(w,b)在某一点的值，我们想要做的就是找到这样的w和b使其对应的成本函数J值，是最小值。
可以看出，成本函数J是一个凸函数，和非凸的函数不一样，非凸函数有很多的局部最优，因此，我们使用为凸函数的J(w，b)，凸函数的这个性质是我们为啥使用这个特定的成本函数J做logistic回归一个很重要的原因。为了找到更好的参数值，我们要做的就是用某初始值，初始化w和b。对于logistic回归而言，几乎是任意的初始化方法都有效，通常用0来初始化，但人们一般不这么做，但是因为函数是凸的，无论在哪里初始化，都应该到达到用一点，或大致相同的点。梯度下降法所做的就是，从初始点开始，朝最抖的下坡方向走一步，在梯度下降一步后，也许就会停在碗的最低端，因为它试着沿着最快下降的方向往下走，这是梯度下降的一次迭代。两次迭代可能就到了最低点，或者需要更多次，隐藏在图上的曲线中，很有希望收敛到这个全局最优解，或接近全局最优解，这张图片阐述了梯度下降法。


![梯度下降法中参数更新规则](http://upload-images.jianshu.io/upload_images/1779926-e2180c68d3bc4947.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
为了更好的说明如何学习w和b，我们将图像的横轴只有w，变成二维坐标轴，图像如左图，右图的公式代表w如何变化的公式。
α表示学习率，学习率可以控制每一次迭代，或者梯度下降法中的步长，后面会讨论，如何选择学习率α；其次d(J(w))/dw这个数是导数(derivative)，这就是对参数w的更新或者说变化量。

当我们开始编写代码，来实现梯度下降，我们会使用到代码中变量名的约定，dw用来表示导数，作为导数的变量名，那么`w:=w-α*dw `（:=代表变化取值），现在我们确保梯度下降法中更新是有用的。
记住导数的定义，是函数在这个点的斜率。对于一开始就很大的参数w来说，每更新一次就会向左移动，向最小值点更靠近，同样的，假设w很小，在最小值的左边，那么斜率为负值，每次迭代就是w加上一个数，也会逐步的向最小值的w0靠近。

我们想知道，用目前参数的情况下函数的斜率朝下降速度最快的方向走。我们知道，为了让成本函数J走下坡路，下一步更新的方向在哪。当前J(w)的梯度下降法只有参数w，在logistic回归中，你的成本函数是一个含有w和b的函数，在这种情况下，梯度下降的内循环就是这里的这个东西，你必须重复的计算，通过`w:=w-α*dw`更新w，通过`b:=b-α*d(J(w,b))/db`更新b。这两个公式是实际更新w和b时所作的操作。当然这里的符号d微分也可以是偏导数花哨的α，表示的是函数在w方向的斜率是多小，当函数有两个以上的变量时，应该使用偏导数符号，计算函数关于其中一个变量的在对应点所对应的斜率。

#### 2.5 导数
#### 2.6 更多关于导数的例子
这两节主要就是关于微积分，导数的内容，通过举例f(x)=3x, f(x)=x^2, f(x)=x^3来说明函数上某一点的导数其实就是在该点的斜率，直线上点的斜率处处相等，但是对于其他非直线的函数，每个点的斜率即导数可能都不相同。
对于f(x)=x^2, f(x)=2x, 当x=2时，f(2)=4, 将x向后移动一点点，比如x=2.001，对应的f(x)=8.004002 约为8.004，发现f(x)增量与x的增量相比，结果为`8.004/2.001=4=2*2`，这里f(x)我们只是近似的等于4倍，实际上，按照导数的定义，是在x点出，增加一个无穷小量，0.001的增量很明显不能表示无穷小，因此在该点处的切线的斜率就为2x。


#### 2.7 计算图
可以说，一个神经网络的计算都是按照前向或者反向传播过程来实现的，首先计算出，神经网络的输出，紧接着进行一个反向传播过程，后者我们用来计算出对应的梯度或者导数，以下流程图解释了，为什么要用这样的方式实现。
为了阐明这个计算过程，举一个比logistc回归更加简单的、不那么正式的神经网络的例子。我们尝试计算函数J,它是关于三个变量a,b,c的函数
`J(a,b,c) = 3(a+b*c)`
计算这个函数，实际上有三个不同的步骤：
1. 计算`b*c`，存在在变量u中，`u=b*c`
2. 计算a+u，存在变量v中，v=a+u
3. 最后输出J，J=`3*v`
我们可以把这个计算过程划成如下的流程图：
![`J(a,b,c)=3(a+b*c)` 计算流程图](http://upload-images.jianshu.io/upload_images/1779926-8bd10dd5a908ef05.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


这种流程图用起来很方便，有不同的或者一些特殊的输出变量时，比如我们想要优化的J。在logistc回归中，J是想要最小化的成本函数，可以看出，通过一个从左到右的过程，你可以计算出J的值，计算导数就是一个从右到左的过程，刚好与从左到右传播的过程相反。

#### 2.8 计算图的导数计算
在使用计算图计算出J的值后，我们研究如果通过计算图计算出函数J的导数，在上个例子中，要计算J对v的导数，怎么做？
在反向传播术中，我们看到如果你想计算最后输出变量的导数，使用你最关心的变量对v的导数，dJ/dv=3, 那么我们就做完了一步反向传播。
那么关于dJ/da呢，也就是说改变a，对J的数值有什么影响呢
```
a=5时，v为11，J为33
a=5.001时，v为11.001,J为33.003
```
那么增加a，如果你把这个5换成某个新值，那么a的改变就会传播到流程图的最右，所以J最后是33.001。所以J的增量是3乘以a的增量，意味着这个导数是3。换句话说，如果你改变了a，那么就改变了v，通过改变v，又会改变J，所以J值的净变化量，当你提升这个值，当你把a值提高一点点，这就是J的净变化量。关于a影响v，v影响J，这在微积分里叫做链式法则，所以
```
dJ/da = dJ/dv * dv/da
```
这样我们就完成了另一步反向传播。


介绍一个新的符号约定，当你编程实现反向传播时，通常会有一个最终输出值是你要关心的，最终的输出变量，你真正想要关心的或者说优化的。在例子中，最终的输出变量是J，就是流程图的最后一个符号，所以有很多的计算尝试计算输出变量的导数，所以d输出变量对于某个变量的导数，我们就用d var命名。因为在python代码里，可以用d(finalvar)/dvar,例如dJ/dvar，但是在这个反向传播过程中，我们都是在对最终变量求它的导数，因此就用dvar来表示这个整体，所以在编程时候，我们就用d var来表示J关于对代码中各种中间量的导数。

#### 2.9 logistc回归中的梯度下降法
在本节我们将讨论如何计算偏导数来实现logistic回归的梯度下降法，它的核心关键点，其中有几个重要的公式用来实现logistc回归的梯度下降法，将使用导数流程图来计算梯度。使用计算图来计算logistc回归的梯度有点大材小用了。

回想一下logistc回归的公式，y hat或者说a的定义是预测出来的值，是logostic回归的输出；y是样本的基本真值即标签值，并且a=sigmoid(z),而z=w^T+b，现在只考虑单个样本的情况，关于该样本的损失函数如第三个公式，L(a,y)是关于y和y hat的函数，实质是w和b。现在写出该样本的偏导数流程图如图2。
![logistc回归关于单个样本的公式集 图1](http://upload-images.jianshu.io/upload_images/5355764-4b073db21781c395.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

假设样本只有两个，特征值x1，x2，为了计算z，我们需要输入参数w1,w2，b，还有样本特征值x1，x2，这几个都是用来计算z的，`z=x1*w1+x2*w2+b`，接下来再计算a即y hat，a=sigmoid(z),最后计算L(a,y)。
![logistc回归中的计算图 图2](http://upload-images.jianshu.io/upload_images/5355764-fe59468351b32e7a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

因此我们在logistc回归中，需要做的就是变换参数w和b的值来最小化损失函数，在前面我们已经经过前向传播步骤在单个训练样本上，计算损失函数，接下来讨论，如何向后传播来计算偏导数，其实就是根据链式求导法则，把每一个变量的偏导数求出来。
```
偏导数求出来如下：
da = dL/da = -y/a + (1-y)/(1-a)
dz = dL/dz = dL/da * da/dz  = a - y
这里需要说明，关于 a=sigmoid(z),如何求da/dz，按正常求导法则得到，求导结果为 e^(-z) * 1/(1+e^(-z)), 发现与原始函数相同，将a带入，因此最后结果，关于sigmid函数求导结果为a(1-a),即da/dz=a(1-a)

向后传播的最后一步，就是看w和b怎么变化
dw1 = dL/dw1 = dL/dz * dz/dw1 = x1 * dz
dw2 = dL/dw2 = dL/dz * dz/dw2 = x2 * dz
db = dz
```

![单个样本logistc回归反向传播手写图](http://upload-images.jianshu.io/upload_images/5355764-c3d296089b684f78.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


因此这就是关于单个样本的梯度下降法，你所需要做的就是这些事情，使用这个公式计算dz，dw1，dw2，db，然后更新w1为 `w1=w1-dw1*α（学习率）`，类似的，更新w2，更新b为`b=b-db*α`, 这就是单个样本实例的一次梯度更新步骤。

现在你知道了怎么计算导数，并且实现了单个训练样本的logistc回归的梯度下降法，但是训练logistc回归模型，不仅仅只有一个训练样本，而是有m个训练样本的整个训练集。下一节将讲这些想法如何应用到整个训练样本集中，而不仅仅只是单个样本。

#### 2.10 m个样本的梯度下降
